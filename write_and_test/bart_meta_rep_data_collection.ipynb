{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as pplt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import gym_bart\n",
    "import itertools\n",
    "from umap import UMAP\n",
    "from plotting_utils import (\n",
    "    set_rc, \n",
    "    add_abc_to_subaxes, \n",
    "    colors, \n",
    "    rgb_colors\n",
    ")\n",
    "from model_evaluation import (\n",
    "    forced_action_evaluate, \n",
    "    meta_bart_callback,\n",
    "    meta_bart_multi_callback,\n",
    "    reshape_parallel_evalu_res,\n",
    "    forced_action_evaluate_multi,\n",
    ")\n",
    "from bart_behavior_analysis import (\n",
    "    plot_1color5fsize,\n",
    "    plot_1colornfsize\n",
    ")\n",
    "from read_experiments import average_runs, load_exp_df\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ppo.envs import make_vec_env\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from bart_representation_analysis import *\n",
    "from bart_compress_visualize_decode import *\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "set_rc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.arange(0.2, 1.01, 0.05)\n",
    "env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "giverew_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'give_rew': True,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "\n",
    "giverew_evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=giverew_env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('data/meta_representation_results', 'rb'))\n",
    "\n",
    "last_sizes = r['last_sizes']\n",
    "unpopped_sizes = r['unpopped_sizes']\n",
    "pop_rate = r['pop_rate']\n",
    "rewards = r['rewards']\n",
    "values = r['values']\n",
    "action_probs = r['action_probs']\n",
    "all_lens = r['all_lens']\n",
    "all_num_balloons = r['all_num_balloons']\n",
    "\n",
    "dec_flow_scores = r['dec_flow_scores']\n",
    "\n",
    "iterators_idxs = r['iterators_idxs']\n",
    "sizes = r['sizes']\n",
    "ramp_f1s = r['ramp_f1s']\n",
    "ramp_indiv_contribs = r['ramp_indiv_contribs']\n",
    "confidence_scores = r['confidence_scores']\n",
    "unconfidence_scores = r['unconfidence_scores']\n",
    "unconfident_points = r['unconfident_points']\n",
    "step_count = r['step_count']\n",
    "all_decision_nodes = r['all_decision_nodes']\n",
    "\n",
    "cluster_regressor_coefs = r['cluster_regressor_coefs']\n",
    "cluster_regressor_scores = r['cluster_regressor_scores']\n",
    "cluster_ks = r['cluster_ks']\n",
    "pca_regressor_coefs = r['pca_regressor_coefs']\n",
    "pca_regressor_scores = r['pca_regressor_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [4:42:35<00:00, 17.66s/it]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "last_sizes = np.zeros(sizes + [17, 50])\n",
    "unpopped_sizes = np.zeros(sizes + [17, 50])\n",
    "pop_rate = np.zeros(sizes + [17])\n",
    "rewards = np.zeros(sizes + [17, 2500])\n",
    "values = np.zeros(sizes + [17, 2500])\n",
    "action_probs = np.zeros(sizes + [17, 2500])\n",
    "all_lens = np.zeros(sizes + [17])\n",
    "all_num_balloons = np.zeros(sizes + [17])\n",
    "balloon_steps = np.full(sizes + [17, 50], -1)\n",
    "button_presses = np.full(sizes + [17, 100], -1)\n",
    "\n",
    "# Decision flow\n",
    "dec_flow_scores = np.zeros(sizes + [2]) # last axis: 0=decision nodes, 1=non-dec nodes\n",
    "\n",
    "# Ramp to threshold F1 scores\n",
    "ramp_f1s = np.zeros(sizes + [6, 11]) \n",
    "ramp_indiv_contribs = np.zeros(sizes + [6, 64])\n",
    "\n",
    "# Confidence scores\n",
    "confidence_scores = np.zeros(sizes)\n",
    "unconfidence_scores = np.zeros(sizes)\n",
    "unconfident_points = np.zeros(sizes)\n",
    "step_count = np.zeros(sizes)\n",
    "all_decision_nodes = np.zeros(sizes + [17, 64])\n",
    "\n",
    "# Cluster and PCA regressor coefs\n",
    "cluster_regressor_coefs = np.zeros(sizes + [6, 3, 64])\n",
    "cluster_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "cluster_ks = np.zeros(sizes + [6])\n",
    "pca_regressor_coefs = np.zeros(sizes + [6, 3, 6])\n",
    "pca_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "\n",
    "for h, i, j, k, l in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    if step_count[h, i, j, k, l] != 0:\n",
    "        continue\n",
    "    \n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chk = chks[l]\n",
    "    \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "    \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "\n",
    "    # Performance\n",
    "    lens = np.array([len(d) for d in res['dones']])\n",
    "    num_balloons = np.array([len(d) for d in res['data']['balloon_step']])\n",
    "    unpop_size = get_sizes(res, obs_rms, last_only=True)\n",
    "    for ep in range(17):\n",
    "        last_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = res['data']['last_size'][ep]\n",
    "        unpopped_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = unpop_size[ep]\n",
    "        pop_rate[h, i, j, k, l, ep] = np.sum(res['data']['popped'][ep]) / num_balloons[ep]\n",
    "        values[h, i, j, k, l, ep, :lens[ep]] = res['values'][ep].reshape(-1)\n",
    "        rewards[h, i, j, k, l, ep, :lens[ep]] = res['rewards'][ep]\n",
    "        action_probs[h, i, j, k, l, ep, :lens[ep]] = res['action_probs'][ep][:, 1]\n",
    "        \n",
    "        presses = np.argwhere(res['actions'][ep].reshape(-1) == 1).reshape(-1)\n",
    "        bsteps = res['data']['balloon_step'][ep]\n",
    "        button_presses[h, i, j, k, l, ep, :len(presses)] = presses\n",
    "        balloon_steps[h, i, j, k, l, ep, :len(bsteps)] = bsteps\n",
    "        \n",
    "    all_lens[h, i, j, k, l] = lens\n",
    "    all_num_balloons[h, i, j, k, l] = num_balloons\n",
    "    \n",
    "    # Decision flow\n",
    "    try:\n",
    "        score, dec_nodes = score_decision_flow(res, model)\n",
    "        dec_flow_scores[h, i, j, k, l] = score.mean(axis=0)\n",
    "        all_decision_nodes[h, i, j, k, l] = dec_nodes\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Confidence scores    \n",
    "    non_presses = (np.vstack(res['actions']) == 0).reshape(-1)\n",
    "    presses = (np.vstack(res['actions']) == 1).reshape(-1)\n",
    "    aps = np.vstack(res['action_probs'])[:, 1]\n",
    "    confidence_scores[h, i, j, k, l] = aps[presses].mean()\n",
    "    unconfidence_scores[h, i, j, k, l] = aps[non_presses].mean()\n",
    "    unconfident_points[h, i, j, k, l] = ((aps > 0.2) & (aps < 0.8)).sum() \n",
    "    step_count[h, i, j, k, l] = len(aps)\n",
    "    \n",
    "    # Ramp to threshold scores\n",
    "    f1_scores, individual_scores = score_logistic_classifiers(res)\n",
    "    ramp_f1s[h, i, j, k, l] = f1_scores\n",
    "    ramp_indiv_contribs[h, i, j, k, l] = individual_scores\n",
    "    \n",
    "    # Cluster and PCA regressor scores\n",
    "    layers = ['shared0', 'shared1', 'actor0', 'actor1', 'critic0', 'critic1']\n",
    "    for z, layer in enumerate(layers):\n",
    "        coefs, scores = compute_regressor_coefficients(res, by_clusters=True, layer=layer)\n",
    "        n_clusters = coefs.shape[1]\n",
    "        cluster_regressor_coefs[h, i, j, k, l, z, :, :n_clusters] = coefs\n",
    "        cluster_regressor_scores[h, i, j, k, l, z] = scores\n",
    "        cluster_ks[h, i, j, k, l, z] = n_clusters\n",
    "\n",
    "        coefs, scores = compute_regressor_coefficients(res, by_clusters=False, layer=layer)\n",
    "        pca_regressor_coefs[h, i, j, k, l, z] = coefs\n",
    "        pca_regressor_scores[h, i, j, k, l, z] = scores\n",
    "    \n",
    "    \n",
    "                \n",
    "pickle.dump({\n",
    "    'last_sizes': last_sizes, \n",
    "    'unpopped_sizes': unpopped_sizes, \n",
    "    'pop_rate': pop_rate, \n",
    "    'rewards': rewards, \n",
    "    'values': values, \n",
    "    'action_probs': action_probs,\n",
    "    'all_lens': all_lens, \n",
    "    'all_num_balloons': all_num_balloons, \n",
    "    'balloon_steps': balloon_steps,\n",
    "    'button_presses': button_presses,\n",
    "    \n",
    "    'dec_flow_scores': dec_flow_scores,\n",
    "    \n",
    "    'iterators_idxs': iterators_idxs,\n",
    "    'sizes': sizes,\n",
    "    'ramp_f1s': ramp_f1s,\n",
    "    'ramp_indiv_contribs': ramp_indiv_contribs,\n",
    "    'confidence_scores': confidence_scores,\n",
    "    'unconfidence_scores': unconfidence_scores,\n",
    "    'unconfident_points': unconfident_points,\n",
    "    'step_count': step_count,\n",
    "    'all_decision_nodes': all_decision_nodes,\n",
    "    \n",
    "    'cluster_regressor_coefs': cluster_regressor_coefs, \n",
    "    'cluster_regressor_scores': cluster_regressor_scores, \n",
    "    'cluster_ks': cluster_ks, \n",
    "    'pca_regressor_coefs': pca_regressor_coefs, \n",
    "    'pca_regressor_scores': pca_regressor_scores, \n",
    "}, open('data/meta_representation_results', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [34:25<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "pop_rate = np.zeros(sizes + [17])\n",
    "\n",
    "for h, i, j, k, l in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chk = chks[l]\n",
    "    \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "    \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "\n",
    "    # Performance\n",
    "    lens = np.array([len(d) for d in res['dones']])\n",
    "    num_balloons = np.array([len(d) for d in res['data']['balloon_step']])\n",
    "    unpop_size = get_sizes(res, obs_rms, last_only=True)\n",
    "    for ep in range(17):\n",
    "        pop_rate[h, i, j, k, l, ep] = np.sum(res['data']['popped'][ep]) / num_balloons[ep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect activation data\n",
    "\n",
    "Here we'll just collect data for the \"first\" checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_lens(targets, ep_lens):\n",
    "    '''\n",
    "    split stacked data by ep lens\n",
    "    '''\n",
    "    cur_idx = 0\n",
    "    split_targets = []\n",
    "    for i in range(len(ep_lens)):\n",
    "        next_idx = cur_idx + ep_lens[i]\n",
    "        done_targets = targets[cur_idx:next_idx]\n",
    "        cur_idx = next_idx\n",
    "\n",
    "        split_targets.append(done_targets)\n",
    "    return split_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 64 but corresponding boolean dimension is 7921",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     57\u001b[0m ep_lens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdones\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     58\u001b[0m k, cluster_activ, labels, kmeans, orientation \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mkmeans_oriented_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrnn_hxs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m all_rnn_hxs[idx] \u001b[38;5;241m=\u001b[39m activ\n\u001b[0;32m     62\u001b[0m all_obs[idx] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\14and\\Desktop\\Work\\github\\bart_ppo\\write_and_test\\bart_compress_visualize_decode.py:288\u001b[0m, in \u001b[0;36mkmeans_oriented_activations\u001b[1;34m(res, layer, require_ap_explained)\u001b[0m\n\u001b[0;32m    285\u001b[0m silhouette_scores\u001b[38;5;241m.\u001b[39mappend(silhouette_avg)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Test how good the cluster compression explains behavior\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m cluster_activations, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_cluster_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkmeans\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m    290\u001b[0m lr\u001b[38;5;241m.\u001b[39mfit(cluster_activations, ap)\n",
      "File \u001b[1;32mc:\\Users\\14and\\Desktop\\Work\\github\\bart_ppo\\write_and_test\\bart_compress_visualize_decode.py:184\u001b[0m, in \u001b[0;36mget_cluster_activations\u001b[1;34m(res, layer, labels, kmeans, k, orientation, random_state, ret_activ)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(labels) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 184\u001b[0m cluster_data \u001b[38;5;241m=\u001b[39m [data_normalized[:, labels \u001b[38;5;241m==\u001b[39m i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k)]\n\u001b[0;32m    185\u001b[0m cluster_activations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([c\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cluster_data])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret_activ:\n",
      "File \u001b[1;32mc:\\Users\\14and\\Desktop\\Work\\github\\bart_ppo\\write_and_test\\bart_compress_visualize_decode.py:184\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(labels) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 184\u001b[0m cluster_data \u001b[38;5;241m=\u001b[39m [\u001b[43mdata_normalized\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k)]\n\u001b[0;32m    185\u001b[0m cluster_activations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([c\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cluster_data])\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret_activ:\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 64 but corresponding boolean dimension is 7921"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = pickle.load(open('data/meta_representation_first_idxs', 'rb'))\n",
    "used_chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "cluster_activations = {}\n",
    "cluster_labels = {}\n",
    "cluster_orientations = {}\n",
    "rnn_hx_influences = {}\n",
    "val_grads = {}\n",
    "action_grads = {}\n",
    "all_rnn_hxs = {}\n",
    "all_ep_lens = {}\n",
    "all_obs = {}\n",
    "\n",
    "for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    idx = (h, i, j, k)\n",
    "    if idx in cluster_activations:\n",
    "        continue\n",
    "    \n",
    "    chk = used_chks[chks[h, i, j, k]]\n",
    "        \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "        \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "    \n",
    "    activ = np.vstack(res['rnn_hxs'])\n",
    "    obs = np.vstack(res['obs'])\n",
    "    ep_lens = [len(d) for d in res['dones']]\n",
    "    k, cluster_activ, labels, kmeans, orientation = \\\n",
    "        kmeans_oriented_activations(res, layer='rnn_hxs')\n",
    "\n",
    "    all_rnn_hxs[idx] = activ\n",
    "    all_obs[idx] = obs\n",
    "    all_ep_lens[idx] = ep_lens\n",
    "    cluster_activations[idx] = cluster_activ\n",
    "    cluster_labels[idx] = labels\n",
    "    cluster_orientations[idx] = orientation\n",
    "\n",
    "    give = (h == 1)\n",
    "    val_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='value', plot=False, give=give)\n",
    "    action_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='action', plot=False, give=give)\n",
    "    rnn_hx_influences[idx] = compute_rnn_hxs_influences(model, res)\n",
    "\n",
    "    \n",
    "    \n",
    "                \n",
    "pickle.dump({\n",
    "    'cluster_activations': cluster_activations,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'cluster_orientations': cluster_orientations,\n",
    "    'rnn_hx_influences': rnn_hx_influences,\n",
    "    'val_grads': val_grads,\n",
    "    'action_grads': action_grads,\n",
    "    'all_rnn_hxs': all_rnn_hxs,\n",
    "    'all_ep_lens': all_ep_lens,\n",
    "    'all_obs': all_obs,\n",
    "}, open('data/meta_rnn_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = pickle.load(open('data/meta_representation_first_idxs', 'rb'))\n",
    "used_chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "ares = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "\n",
    "cluster_activations = ares['cluster_activations']\n",
    "cluster_labels = ares['cluster_labels']\n",
    "cluster_orientations = ares['cluster_orientations']\n",
    "rnn_hx_influences = ares['rnn_hx_influences']\n",
    "val_grads = ares['val_grads']\n",
    "action_grads = ares['action_grads']\n",
    "all_rnn_hxs = ares['all_rnn_hxs']\n",
    "all_ep_lens = ares['all_ep_lens']\n",
    "all_obs = ares['all_obs']\n",
    "\n",
    "# for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "for h, i, j, k in [(1, 0, 0, 0)]:\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    idx = (h, i, j, k)\n",
    "    \n",
    "    chk = used_chks[chks[h, i, j, k]]\n",
    "        \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "        \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "    \n",
    "    activ = np.vstack(res['rnn_hxs'])\n",
    "    obs = np.vstack(res['obs'])\n",
    "    ep_lens = [len(d) for d in res['dones']]\n",
    "    k, cluster_activ, labels, kmeans, orientation = \\\n",
    "        kmeans_oriented_activations(res, layer='rnn_hxs')\n",
    "\n",
    "    all_rnn_hxs[idx] = activ\n",
    "    all_obs[idx] = obs\n",
    "    all_ep_lens[idx] = ep_lens\n",
    "    cluster_activations[idx] = cluster_activ\n",
    "    cluster_labels[idx] = labels\n",
    "    cluster_orientations[idx] = orientation\n",
    "\n",
    "    give = (h == 1)\n",
    "    val_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='value', plot=False, give=give)\n",
    "    action_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='action', plot=False, give=give)\n",
    "    rnn_hx_influences[idx] = compute_rnn_hxs_influences(model, res)\n",
    "\n",
    "    \n",
    "pickle.dump({\n",
    "    'cluster_activations': cluster_activations,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'cluster_orientations': cluster_orientations,\n",
    "    'rnn_hx_influences': rnn_hx_influences,\n",
    "    'val_grads': val_grads,\n",
    "    'action_grads': action_grads,\n",
    "    'all_rnn_hxs': all_rnn_hxs,\n",
    "    'all_ep_lens': all_ep_lens,\n",
    "    'all_obs': all_obs,\n",
    "}, open('data/meta_rnn_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [33:54<00:00, 16.96s/it]\n"
     ]
    }
   ],
   "source": [
    "ares = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "\n",
    "rnn_hx_influences2 = {}\n",
    "idxs = select_chks_by_dimension()\n",
    "for idx in tqdm(idxs):\n",
    "    model, obs_rms = metabart_model_load(idx)\n",
    "    obs = ares['all_obs'][idx[:4]]\n",
    "    lens = ares['all_ep_lens'][idx[:4]]\n",
    "    rnn_hxs = ares['all_rnn_hxs'][idx[:4]]\n",
    "\n",
    "    obs = split_by_lens(obs, lens)\n",
    "    rnn_hxs = split_by_lens(rnn_hxs, lens)\n",
    "    r = {'rnn_hxs': rnn_hxs,\n",
    "        'obs': obs}\n",
    "    infl = compute_rnn_hxs_influences(model, r, max_unroll=4, nsteps=300)\n",
    "    rnn_hx_influences2[idx[:4]] = infl\n",
    "\n",
    "ares['rnn_hx_influences2'] = rnn_hx_influences2\n",
    "\n",
    "pickle.dump(ares, open('data/meta_rnn_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect limited forced activation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "balloon_limits, forced_actions = pickle.load(open('data/metabart_forced_lim_acts', 'rb'))\n",
    "env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0,\n",
    "                            'pop_noise': 0, 'fix_sizes_per_balloon': True,\n",
    "                            'fix_sizes': s} for s in balloon_limits]\n",
    "give_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'give_rew': True,\n",
    "                            'pop_noise': 0, 'fix_sizes_per_balloon': True,\n",
    "                            'fix_sizes': s} for s in balloon_limits]\n",
    "\n",
    "force_evalu_ = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=env_kwargs, \n",
    "                forced_actions=forced_actions,\n",
    "                num_processes=4,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "force_give_evalu_ = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=give_env_kwargs, \n",
    "                forced_actions=forced_actions,\n",
    "                num_processes=4,\n",
    "                seed=2,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "\n",
    "def force_evalu(model, obs_rms, give_rew=False):\n",
    "    if give_rew:\n",
    "        res = force_give_evalu_(model, obs_rms)\n",
    "    else:\n",
    "        res = force_evalu_(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [19:49<00:00,  9.92s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = pickle.load(open('data/meta_representation_first_idxs', 'rb'))\n",
    "used_chks = np.arange(10, 243, 30)\n",
    "\n",
    "# precollected clusters\n",
    "ares = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "cluster_activations = {}\n",
    "rnn_hx_influences = {}\n",
    "val_grads = {}\n",
    "action_grads = {}\n",
    "all_rnn_hxs = {}\n",
    "\n",
    "for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    idx = (h, i, j, k)\n",
    "    if idx in cluster_activations:\n",
    "        continue\n",
    "    \n",
    "    chk = used_chks[chks[h, i, j, k]]\n",
    "        \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "        \n",
    "    give = True if h == 1 else False\n",
    "    res = force_evalu(model, obs_rms, give_rew=give)\n",
    "    \n",
    "    activ = np.vstack(res['rnn_hxs'])\n",
    "\n",
    "    labels = ares['cluster_labels'][idx]\n",
    "    orientation = ares['cluster_orientations'][idx]\n",
    "    cluster_activ, _, _ = get_cluster_activations(res, 'rnn_hxs', labels, \n",
    "                                                  orientation=orientation)\n",
    "    \n",
    "    all_rnn_hxs[idx] = activ\n",
    "    cluster_activations[idx] = cluster_activ\n",
    "    val_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='value', plot=False, give=give)\n",
    "    action_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='action', plot=False, give=give)\n",
    "    rnn_hx_influences[idx] = compute_rnn_hxs_influences(model, res)\n",
    "\n",
    "                \n",
    "pickle.dump({\n",
    "    'cluster_activations': cluster_activations,\n",
    "    'rnn_hx_influences': rnn_hx_influences,\n",
    "    'val_grads': val_grads,\n",
    "    'action_grads': action_grads,\n",
    "    'all_rnn_hxs': all_rnn_hxs,\n",
    "}, open('data/meta_rnn_forced_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14and\\anaconda3\\envs\\bart\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.balloon_mean_sizes to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.balloon_mean_sizes` for environment variables or `env.get_wrapper_attr('balloon_mean_sizes')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Add run results data to forced activations\n",
    "'''\n",
    "\n",
    "forced_ares = pickle.load(open('data/meta_rnn_forced_gradients', 'rb'))\n",
    "\n",
    "idx = (0, 0, 0, 0)\n",
    "idx = select_chks([idx])[0]\n",
    "model, obs_rms = metabart_model_load(idx)\n",
    "r = force_evalu(model, obs_rms, give_rew=False)\n",
    "\n",
    "idx = (1, 0, 0, 0)\n",
    "idx = select_chks([idx])[0]\n",
    "model, obs_rms = metabart_model_load(idx)\n",
    "r_give = force_evalu(model, obs_rms, give_rew=True)\n",
    "\n",
    "del r['activations']\n",
    "del r['rnn_hxs']\n",
    "del r['action_log_probs']\n",
    "del r['action_probs']\n",
    "del r_give['activations']\n",
    "del r_give['rnn_hxs']\n",
    "del r_give['action_log_probs']\n",
    "del r_give['action_probs']\n",
    "\n",
    "forced_ares['give_res'] = r_give\n",
    "forced_ares['res'] = r\n",
    "\n",
    "pickle.dump(forced_ares, open('data/meta_rnn_forced_gradients', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
