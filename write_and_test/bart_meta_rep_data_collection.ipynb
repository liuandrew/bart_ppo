{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as pplt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import gym_bart\n",
    "import itertools\n",
    "from umap import UMAP\n",
    "from plotting_utils import (\n",
    "    set_rc, \n",
    "    add_abc_to_subaxes, \n",
    "    colors, \n",
    "    rgb_colors\n",
    ")\n",
    "from model_evaluation import (\n",
    "    forced_action_evaluate, \n",
    "    meta_bart_callback,\n",
    "    meta_bart_multi_callback,\n",
    "    reshape_parallel_evalu_res,\n",
    "    forced_action_evaluate_multi,\n",
    ")\n",
    "from bart_behavior_analysis import (\n",
    "    plot_1color5fsize,\n",
    "    plot_1colornfsize\n",
    ")\n",
    "from bart_single_node_responses import calculate_all_single_node_characteristics\n",
    "from read_experiments import average_runs, load_exp_df\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ppo.envs import make_vec_env\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from bart_representation_analysis import *\n",
    "from bart_compress_visualize_decode import *\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "set_rc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.arange(0.2, 1.01, 0.05)\n",
    "env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "giverew_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'give_rew': True,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "fixprev_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'fix_prev_action_bug': True,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "\n",
    "giverew_evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=giverew_env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "fixprev_evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=fixprev_env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('data/meta_representation_results', 'rb'))\n",
    "\n",
    "last_sizes = r['last_sizes']\n",
    "unpopped_sizes = r['unpopped_sizes']\n",
    "pop_rate = r['pop_rate']\n",
    "rewards = r['rewards']\n",
    "values = r['values']\n",
    "action_probs = r['action_probs']\n",
    "all_lens = r['all_lens']\n",
    "all_num_balloons = r['all_num_balloons']\n",
    "\n",
    "dec_flow_scores = r['dec_flow_scores']\n",
    "\n",
    "iterators_idxs = r['iterators_idxs']\n",
    "sizes = r['sizes']\n",
    "ramp_f1s = r['ramp_f1s']\n",
    "ramp_indiv_contribs = r['ramp_indiv_contribs']\n",
    "confidence_scores = r['confidence_scores']\n",
    "unconfidence_scores = r['unconfidence_scores']\n",
    "unconfident_points = r['unconfident_points']\n",
    "step_count = r['step_count']\n",
    "all_decision_nodes = r['all_decision_nodes']\n",
    "\n",
    "cluster_regressor_coefs = r['cluster_regressor_coefs']\n",
    "cluster_regressor_scores = r['cluster_regressor_scores']\n",
    "cluster_ks = r['cluster_ks']\n",
    "pca_regressor_coefs = r['pca_regressor_coefs']\n",
    "pca_regressor_scores = r['pca_regressor_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "give_rew = ['', 'giverew_', 'fixprev_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "last_sizes = np.zeros(sizes + [17, 50])\n",
    "unpopped_sizes = np.zeros(sizes + [17, 50])\n",
    "pop_rate = np.zeros(sizes + [17])\n",
    "rewards = np.zeros(sizes + [17, 2500])\n",
    "values = np.zeros(sizes + [17, 2500])\n",
    "action_probs = np.zeros(sizes + [17, 2500])\n",
    "all_lens = np.zeros(sizes + [17])\n",
    "all_num_balloons = np.zeros(sizes + [17])\n",
    "balloon_steps = np.full(sizes + [17, 50], -1)\n",
    "button_presses = np.full(sizes + [17, 100], -1)\n",
    "\n",
    "# Decision flow\n",
    "dec_flow_scores = np.zeros(sizes + [2]) # last axis: 0=decision nodes, 1=non-dec nodes\n",
    "\n",
    "# Ramp to threshold F1 scores\n",
    "ramp_f1s = np.zeros(sizes + [6, 11]) \n",
    "ramp_indiv_contribs = np.zeros(sizes + [6, 64])\n",
    "\n",
    "# Confidence scores\n",
    "confidence_scores = np.zeros(sizes)\n",
    "unconfidence_scores = np.zeros(sizes)\n",
    "unconfident_points = np.zeros(sizes)\n",
    "step_count = np.zeros(sizes)\n",
    "all_decision_nodes = np.zeros(sizes + [17, 64])\n",
    "\n",
    "# Cluster and PCA regressor coefs\n",
    "cluster_regressor_coefs = np.zeros(sizes + [6, 3, 64])\n",
    "cluster_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "cluster_ks = np.zeros(sizes + [6])\n",
    "pca_regressor_coefs = np.zeros(sizes + [6, 3, 6])\n",
    "pca_regressor_scores = np.zeros(sizes + [6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('data/meta_representation_results', 'rb'))\n",
    "\n",
    "last_sizes[:2] = r['last_sizes'][:2]\n",
    "unpopped_sizes[:2] = r['unpopped_sizes'][:2]\n",
    "pop_rate[:2] = r['pop_rate'][:2]\n",
    "rewards[:2] = r['rewards'][:2]\n",
    "values[:2] = r['values'][:2]\n",
    "action_probs[:2] = r['action_probs'][:2]\n",
    "all_lens[:2] = r['all_lens'][:2]\n",
    "all_num_balloons[:2] = r['all_num_balloons'][:2]\n",
    "\n",
    "dec_flow_scores[:2] = r['dec_flow_scores'][:2]\n",
    "\n",
    "iterators_idxs[:2] = r['iterators_idxs'][:2]\n",
    "sizes[:2] = r['sizes'][:2]\n",
    "ramp_f1s[:2] = r['ramp_f1s'][:2]\n",
    "ramp_indiv_contribs[:2] = r['ramp_indiv_contribs'][:2]\n",
    "confidence_scores[:2] = r['confidence_scores'][:2]\n",
    "unconfidence_scores[:2] = r['unconfidence_scores'][:2]\n",
    "unconfident_points[:2] = r['unconfident_points'][:2]\n",
    "step_count[:2] = r['step_count'][:2]\n",
    "all_decision_nodes[:2] = r['all_decision_nodes'][:2]\n",
    "\n",
    "cluster_regressor_coefs[:2] = r['cluster_regressor_coefs'][:2]\n",
    "cluster_regressor_scores[:2] = r['cluster_regressor_scores'][:2]\n",
    "cluster_ks[:2] = r['cluster_ks'][:2]\n",
    "pca_regressor_coefs[:2] = r['pca_regressor_coefs'][:2]\n",
    "pca_regressor_scores[:2] = r['pca_regressor_scores'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1440/1440 [18:48:36<00:00, 47.03s/it]       \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_', 'fixprev_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# # Performance\n",
    "# last_sizes = np.zeros(sizes + [17, 50])\n",
    "# unpopped_sizes = np.zeros(sizes + [17, 50])\n",
    "# pop_rate = np.zeros(sizes + [17])\n",
    "# rewards = np.zeros(sizes + [17, 2500])\n",
    "# values = np.zeros(sizes + [17, 2500])\n",
    "# action_probs = np.zeros(sizes + [17, 2500])\n",
    "# all_lens = np.zeros(sizes + [17])\n",
    "# all_num_balloons = np.zeros(sizes + [17])\n",
    "balloon_steps = np.full(sizes + [17, 50], -1)\n",
    "button_presses = np.full(sizes + [17, 100], -1)\n",
    "\n",
    "# # Decision flow\n",
    "# dec_flow_scores = np.zeros(sizes + [2]) # last axis: 0=decision nodes, 1=non-dec nodes\n",
    "\n",
    "# # Ramp to threshold F1 scores\n",
    "# ramp_f1s = np.zeros(sizes + [6, 11]) \n",
    "# ramp_indiv_contribs = np.zeros(sizes + [6, 64])\n",
    "\n",
    "# # Confidence scores\n",
    "# confidence_scores = np.zeros(sizes)\n",
    "# unconfidence_scores = np.zeros(sizes)\n",
    "# unconfident_points = np.zeros(sizes)\n",
    "# step_count = np.zeros(sizes)\n",
    "# all_decision_nodes = np.zeros(sizes + [17, 64])\n",
    "\n",
    "# # Cluster and PCA regressor coefs\n",
    "# cluster_regressor_coefs = np.zeros(sizes + [6, 3, 64])\n",
    "# cluster_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "# cluster_ks = np.zeros(sizes + [6])\n",
    "# pca_regressor_coefs = np.zeros(sizes + [6, 3, 6])\n",
    "# pca_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "\n",
    "for h, i, j, k, l in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    if step_count[h, i, j, k, l] != 0:\n",
    "        continue\n",
    "    \n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chk = chks[l]\n",
    "    \n",
    "    if h in [1, 2] and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "    \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    elif h == 2:\n",
    "        res = fixprev_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "\n",
    "    # Performance\n",
    "    lens = np.array([len(d) for d in res['dones']])\n",
    "    num_balloons = np.array([len(d) for d in res['data']['balloon_step']])\n",
    "    unpop_size = get_sizes(res, obs_rms, last_only=True)\n",
    "    for ep in range(17):\n",
    "        last_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = res['data']['last_size'][ep]\n",
    "        unpopped_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = unpop_size[ep]\n",
    "        pop_rate[h, i, j, k, l, ep] = np.sum(res['data']['popped'][ep]) / num_balloons[ep]\n",
    "        values[h, i, j, k, l, ep, :lens[ep]] = res['values'][ep].reshape(-1)\n",
    "        rewards[h, i, j, k, l, ep, :lens[ep]] = res['rewards'][ep]\n",
    "        action_probs[h, i, j, k, l, ep, :lens[ep]] = res['action_probs'][ep][:, 1]\n",
    "        \n",
    "        presses = np.argwhere(res['actions'][ep].reshape(-1) == 1).reshape(-1)\n",
    "        bsteps = res['data']['balloon_step'][ep]\n",
    "        button_presses[h, i, j, k, l, ep, :len(presses)] = presses\n",
    "        balloon_steps[h, i, j, k, l, ep, :len(bsteps)] = bsteps\n",
    "        \n",
    "    all_lens[h, i, j, k, l] = lens\n",
    "    all_num_balloons[h, i, j, k, l] = num_balloons\n",
    "    \n",
    "    # Decision flow\n",
    "    try:\n",
    "        score, dec_nodes = score_decision_flow(res, model)\n",
    "        dec_flow_scores[h, i, j, k, l] = score.mean(axis=0)\n",
    "        all_decision_nodes[h, i, j, k, l] = dec_nodes\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Confidence scores    \n",
    "    non_presses = (np.vstack(res['actions']) == 0).reshape(-1)\n",
    "    presses = (np.vstack(res['actions']) == 1).reshape(-1)\n",
    "    aps = np.vstack(res['action_probs'])[:, 1]\n",
    "    confidence_scores[h, i, j, k, l] = aps[presses].mean()\n",
    "    unconfidence_scores[h, i, j, k, l] = aps[non_presses].mean()\n",
    "    unconfident_points[h, i, j, k, l] = ((aps > 0.2) & (aps < 0.8)).sum() \n",
    "    step_count[h, i, j, k, l] = len(aps)\n",
    "    \n",
    "    # Ramp to threshold scores\n",
    "    f1_scores, individual_scores = score_logistic_classifiers(res)\n",
    "    ramp_f1s[h, i, j, k, l] = f1_scores\n",
    "    ramp_indiv_contribs[h, i, j, k, l] = individual_scores\n",
    "    \n",
    "    # Cluster and PCA regressor scores\n",
    "    layers = ['shared0', 'shared1', 'actor0', 'actor1', 'critic0', 'critic1']\n",
    "    for z, layer in enumerate(layers):\n",
    "        try:\n",
    "            coefs, scores = compute_regressor_coefficients(res, by_clusters=True, layer=layer)\n",
    "            n_clusters = coefs.shape[1]\n",
    "            cluster_regressor_coefs[h, i, j, k, l, z, :, :n_clusters] = coefs\n",
    "            cluster_regressor_scores[h, i, j, k, l, z] = scores\n",
    "            cluster_ks[h, i, j, k, l, z] = n_clusters\n",
    "        \n",
    "            coefs, scores = compute_regressor_coefficients(res, by_clusters=False, layer=layer)\n",
    "            pca_regressor_coefs[h, i, j, k, l, z] = coefs\n",
    "            pca_regressor_scores[h, i, j, k, l, z] = scores\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "                \n",
    "pickle.dump({\n",
    "    'last_sizes': last_sizes, \n",
    "    'unpopped_sizes': unpopped_sizes, \n",
    "    'pop_rate': pop_rate, \n",
    "    'rewards': rewards, \n",
    "    'values': values, \n",
    "    'action_probs': action_probs,\n",
    "    'all_lens': all_lens, \n",
    "    'all_num_balloons': all_num_balloons, \n",
    "    'balloon_steps': balloon_steps,\n",
    "    'button_presses': button_presses,\n",
    "    \n",
    "    'dec_flow_scores': dec_flow_scores,\n",
    "    \n",
    "    'iterators_idxs': iterators_idxs,\n",
    "    'sizes': sizes,\n",
    "    'ramp_f1s': ramp_f1s,\n",
    "    'ramp_indiv_contribs': ramp_indiv_contribs,\n",
    "    'confidence_scores': confidence_scores,\n",
    "    'unconfidence_scores': unconfidence_scores,\n",
    "    'unconfident_points': unconfident_points,\n",
    "    'step_count': step_count,\n",
    "    'all_decision_nodes': all_decision_nodes,\n",
    "    \n",
    "    'cluster_regressor_coefs': cluster_regressor_coefs, \n",
    "    'cluster_regressor_scores': cluster_regressor_scores, \n",
    "    'cluster_ks': cluster_ks, \n",
    "    'pca_regressor_coefs': pca_regressor_coefs, \n",
    "    'pca_regressor_scores': pca_regressor_scores, \n",
    "}, open('data/meta_representation_results', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect activation data\n",
    "\n",
    "Here we'll just collect data for the \"first\" checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_lens(targets, ep_lens):\n",
    "    '''\n",
    "    split stacked data by ep lens\n",
    "    '''\n",
    "    cur_idx = 0\n",
    "    split_targets = []\n",
    "    for i in range(len(ep_lens)):\n",
    "        next_idx = cur_idx + ep_lens[i]\n",
    "        done_targets = targets[cur_idx:next_idx]\n",
    "        cur_idx = next_idx\n",
    "\n",
    "        split_targets.append(done_targets)\n",
    "    return split_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "res = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "cluster_activations = res['cluster_activations']\n",
    "cluster_labels = res['cluster_labels']\n",
    "cluster_orientations = res['cluster_orientations']\n",
    "rnn_hx_influences = res['rnn_hx_influences']\n",
    "val_grads = res['val_grads']\n",
    "action_grads = res['action_grads']\n",
    "all_rnn_hxs = res['all_rnn_hxs']\n",
    "all_ep_lens = res['all_ep_lens']\n",
    "all_obs = res['all_obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [13:50<00:00,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Collect activation data by running selected first >275 score checkpoints\n",
    "on evaluation suite (agent's own actions), collecting activities, node clusters,\n",
    "and measuring their influence on value and policy outputs\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_', 'fixprev_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = pickle.load(open('data/meta_representation_first_idxs', 'rb'))\n",
    "used_chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "# cluster_activations = {}\n",
    "# cluster_labels = {}\n",
    "# cluster_orientations = {}\n",
    "# rnn_hx_influences = {}\n",
    "# val_grads = {}\n",
    "# action_grads = {}\n",
    "# all_rnn_hxs = {}\n",
    "# all_ep_lens = {}\n",
    "# all_obs = {}\n",
    "\n",
    "for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    idx = (h, i, j, k)\n",
    "    if idx in cluster_activations:\n",
    "        continue\n",
    "    \n",
    "    chk = used_chks[chks[h, i, j, k]]\n",
    "        \n",
    "    if h in [1, 2] and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "        \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    elif h == 2:\n",
    "        res = fixprev_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "    \n",
    "    activ = np.vstack(res['rnn_hxs'])\n",
    "    obs = np.vstack(res['obs'])\n",
    "    ep_lens = [len(d) for d in res['dones']]\n",
    "    k, cluster_activ, labels, kmeans, orientation = \\\n",
    "        kmeans_oriented_activations(res, layer='rnn_hxs')\n",
    "\n",
    "    all_rnn_hxs[idx] = activ\n",
    "    all_obs[idx] = obs\n",
    "    all_ep_lens[idx] = ep_lens\n",
    "    cluster_activations[idx] = cluster_activ\n",
    "    cluster_labels[idx] = labels\n",
    "    cluster_orientations[idx] = orientation\n",
    "\n",
    "    give = (h == 1)\n",
    "    val_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='value', plot=False, give=give)\n",
    "    action_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='action', plot=False, give=give)\n",
    "    rnn_hx_influences[idx] = compute_rnn_hxs_influences(model, res)\n",
    "\n",
    "    \n",
    "    \n",
    "                \n",
    "pickle.dump({\n",
    "    'cluster_activations': cluster_activations,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'cluster_orientations': cluster_orientations,\n",
    "    'rnn_hx_influences': rnn_hx_influences,\n",
    "    'val_grads': val_grads,\n",
    "    'action_grads': action_grads,\n",
    "    'all_rnn_hxs': all_rnn_hxs,\n",
    "    'all_ep_lens': all_ep_lens,\n",
    "    'all_obs': all_obs,\n",
    "}, open('data/meta_rnn_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [33:54<00:00, 16.96s/it]\n"
     ]
    }
   ],
   "source": [
    "ares = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "\n",
    "rnn_hx_influences2 = {}\n",
    "idxs = select_chks_by_dimension()\n",
    "for idx in tqdm(idxs):\n",
    "    model, obs_rms = metabart_model_load(idx)\n",
    "    obs = ares['all_obs'][idx[:4]]\n",
    "    lens = ares['all_ep_lens'][idx[:4]]\n",
    "    rnn_hxs = ares['all_rnn_hxs'][idx[:4]]\n",
    "\n",
    "    obs = split_by_lens(obs, lens)\n",
    "    rnn_hxs = split_by_lens(rnn_hxs, lens)\n",
    "    r = {'rnn_hxs': rnn_hxs,\n",
    "        'obs': obs}\n",
    "    infl = compute_rnn_hxs_influences(model, r, max_unroll=4, nsteps=300)\n",
    "    rnn_hx_influences2[idx[:4]] = infl\n",
    "\n",
    "ares['rnn_hx_influences2'] = rnn_hx_influences2\n",
    "\n",
    "pickle.dump(ares, open('data/meta_rnn_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect limited forced activation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "balloon_limits, forced_actions = pickle.load(open('data/metabart_forced_lim_acts', 'rb'))\n",
    "env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0,\n",
    "                            'pop_noise': 0, 'fix_sizes_per_balloon': True,\n",
    "                            'fix_sizes': s} for s in balloon_limits]\n",
    "give_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'give_rew': True,\n",
    "                            'pop_noise': 0, 'fix_sizes_per_balloon': True,\n",
    "                            'fix_sizes': s} for s in balloon_limits]\n",
    "fixprev_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'fix_prev_action_bug': True,\n",
    "                            'pop_noise': 0, 'fix_sizes_per_balloon': True,\n",
    "                            'fix_sizes': s} for s in balloon_limits]\n",
    "\n",
    "# Note, seed does not matter since we are fixing the exact sizes of each balloon that appears\n",
    "force_evalu_ = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=env_kwargs, \n",
    "                forced_actions=forced_actions,\n",
    "                num_processes=4,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "force_give_evalu_ = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=give_env_kwargs, \n",
    "                forced_actions=forced_actions,\n",
    "                num_processes=4,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "force_fixprev_evalu_ = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=fixprev_env_kwargs, \n",
    "                forced_actions=forced_actions,\n",
    "                num_processes=4,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "\n",
    "def force_evalu(model, obs_rms, give_rew=False, fixprev=False):\n",
    "    if give_rew and fixprev:\n",
    "        print('Warning: both give_rew and fixprev are set to True, but only give_rew will activate')\n",
    "    if give_rew:\n",
    "        res = force_give_evalu_(model, obs_rms)\n",
    "    elif fixprev:\n",
    "        res = force_fixprev_evalu_(model, obs_rms)\n",
    "    else:\n",
    "        res = force_evalu_(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "res = pickle.load(open('data/meta_rnn_forced_gradients', 'rb'))\n",
    "cluster_activations = res['cluster_activations']\n",
    "rnn_hx_influences = res['rnn_hx_influences']\n",
    "val_grads = res['val_grads']\n",
    "action_grads = res['action_grads']\n",
    "all_rnn_hxs = res['all_rnn_hxs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [09:19<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_', 'fixprev_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = pickle.load(open('data/meta_representation_first_idxs', 'rb'))\n",
    "used_chks = np.arange(10, 243, 30)\n",
    "\n",
    "# precollected clusters\n",
    "ares = pickle.load(open('data/meta_rnn_gradients', 'rb'))\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "# cluster_activations = {}\n",
    "# rnn_hx_influences = {}\n",
    "# val_grads = {}\n",
    "# action_grads = {}\n",
    "# all_rnn_hxs = {}\n",
    "\n",
    "for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    idx = (h, i, j, k)\n",
    "    if idx in cluster_activations:\n",
    "        continue\n",
    "    \n",
    "    chk = used_chks[chks[h, i, j, k]]\n",
    "        \n",
    "    if h in [1, 2] and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "        \n",
    "    give = True if h == 1 else False\n",
    "    fixprev = True if h == 2 else False\n",
    "    res = force_evalu(model, obs_rms, give_rew=give, fixprev=fixprev)\n",
    "    \n",
    "    activ = np.vstack(res['rnn_hxs'])\n",
    "\n",
    "    labels = ares['cluster_labels'][idx]\n",
    "    orientation = ares['cluster_orientations'][idx]\n",
    "    cluster_activ, _, _ = get_cluster_activations(res, 'rnn_hxs', labels, \n",
    "                                                  orientation=orientation)\n",
    "    \n",
    "    all_rnn_hxs[idx] = activ\n",
    "    cluster_activations[idx] = cluster_activ\n",
    "    val_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='value', plot=False, give=give)\n",
    "    action_grads[idx] = test_integrated_gradients(model, obs_rms, \n",
    "                                               res, test='action', plot=False, give=give)\n",
    "    rnn_hx_influences[idx] = compute_rnn_hxs_influences(model, res)\n",
    "\n",
    "                \n",
    "pickle.dump({\n",
    "    'cluster_activations': cluster_activations,\n",
    "    'rnn_hx_influences': rnn_hx_influences,\n",
    "    'val_grads': val_grads,\n",
    "    'action_grads': action_grads,\n",
    "    'all_rnn_hxs': all_rnn_hxs,\n",
    "}, open('data/meta_rnn_forced_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add run results data to forced activations\n",
    "'''\n",
    "\n",
    "forced_ares = pickle.load(open('data/meta_rnn_forced_gradients', 'rb'))\n",
    "\n",
    "idx = (0, 0, 0, 0)\n",
    "idx = select_chks([idx])[0]\n",
    "model, obs_rms = metabart_model_load(idx)\n",
    "r = force_evalu(model, obs_rms, give_rew=False)\n",
    "\n",
    "idx = (1, 0, 0, 0)\n",
    "idx = select_chks([idx])[0]\n",
    "model, obs_rms = metabart_model_load(idx)\n",
    "r_give = force_evalu(model, obs_rms, give_rew=True)\n",
    "\n",
    "idx = (2, 0, 0, 0)\n",
    "idx = select_chks([idx])[0]\n",
    "model, obs_rms = metabart_model_load(idx)\n",
    "r_fixprev = force_evalu(model, obs_rms, fixprev=True)\n",
    "\n",
    "del r['activations']\n",
    "del r['rnn_hxs']\n",
    "del r['action_log_probs']\n",
    "del r['action_probs']\n",
    "del r_give['activations']\n",
    "del r_give['rnn_hxs']\n",
    "del r_give['action_log_probs']\n",
    "del r_give['action_probs']\n",
    "del r_fixprev['activations']\n",
    "del r_fixprev['rnn_hxs']\n",
    "del r_fixprev['action_log_probs']\n",
    "del r_fixprev['action_probs']\n",
    "\n",
    "forced_ares['fixprev_res'] = r_fixprev\n",
    "forced_ares['give_res'] = r_give\n",
    "forced_ares['res'] = r\n",
    "\n",
    "pickle.dump(forced_ares, open('data/meta_rnn_forced_gradients', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual node stimulus experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_', 'fixprev_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "response_types = np.zeros(sizes + [64])\n",
    "turning_points = np.zeros(sizes + [64])\n",
    "lr_sens_bias = np.zeros(sizes + [64])\n",
    "size_sensitivities = np.zeros(sizes + [6, 20, 64])\n",
    "pop_sensitivities = np.zeros(sizes + [20, 64])\n",
    "reversals = np.zeros(sizes + [64])\n",
    "\n",
    "for h, i, j, k in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    if h == 0:\n",
    "        continue\n",
    "    if h == 1:\n",
    "        give = True\n",
    "        fixprev = False\n",
    "    elif h == 2:\n",
    "        give = False\n",
    "        fixprev = True\n",
    "    else:\n",
    "        give = False\n",
    "        fixprev = False\n",
    "    idx = (h, i, j, k)\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chkidx = select_chks([idx])[0]\n",
    "    # print(idx, chkidx)\n",
    "    model, obs_rms = metabart_model_load(chkidx)\n",
    "    \n",
    "    r = calculate_all_single_node_characteristics(model, obs_rms, give=give, fix_bug=fixprev,\n",
    "                                                  recovery_diffs=True)\n",
    "    response_types[idx] = r[0]\n",
    "    turning_points[idx] = r[1]\n",
    "    lr_sens_bias[idx] = r[2]\n",
    "    size_sensitivities[idx] = r[3]\n",
    "    pop_sensitivities[idx] = r[4]\n",
    "    reversals[idx] = r[5]\n",
    "    \n",
    "pickle.dump({\n",
    "    'response_types': response_types,\n",
    "    'turning_points': turning_points,\n",
    "    'lr_sens_bias': lr_sens_bias,\n",
    "    'size_sensitivities': size_sensitivities,\n",
    "    'pop_sensitivities': pop_sensitivities,\n",
    "    'reversals': reversals,\n",
    "}, open('data/meta_single_node_characteristics', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
