{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import proplot as pplt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import gym_bart\n",
    "import itertools\n",
    "from umap import UMAP\n",
    "from plotting_utils import (\n",
    "    set_rc, \n",
    "    add_abc_to_subaxes, \n",
    "    colors, \n",
    "    rgb_colors\n",
    ")\n",
    "from model_evaluation import (\n",
    "    forced_action_evaluate, \n",
    "    meta_bart_callback,\n",
    "    meta_bart_multi_callback,\n",
    "    reshape_parallel_evalu_res,\n",
    "    forced_action_evaluate_multi,\n",
    ")\n",
    "from bart_behavior_analysis import (\n",
    "    plot_1color5fsize,\n",
    "    plot_1colornfsize\n",
    ")\n",
    "from read_experiments import average_runs, load_exp_df\n",
    "import re\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from ppo.envs import make_vec_env\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from bart_representation_analysis import *\n",
    "from bart_compress_visualize_decode import *\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "set_rc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.arange(0.2, 1.01, 0.05)\n",
    "env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "giverew_env_kwargs = [{'meta_setup': 1, 'colors_used': 1, \n",
    "                            'max_steps': 2500, 'num_balloons': 50,\n",
    "                            'inflate_noise': 0, 'give_rew': True,\n",
    "                            'fix_sizes': [0, s, 0]} for s in size]\n",
    "\n",
    "evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)\n",
    "\n",
    "giverew_evalu = partial(forced_action_evaluate_multi, data_callback=meta_bart_multi_callback,\n",
    "                env_name=\"BartMetaEnv\", num_episodes=1, \n",
    "                env_kwargs=giverew_env_kwargs, \n",
    "                num_processes=17,\n",
    "                seed=1,\n",
    "                deterministic=False,\n",
    "                with_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('data/meta_representation_results', 'rb'))\n",
    "\n",
    "last_sizes = r['last_sizes']\n",
    "unpopped_sizes = r['unpopped_sizes']\n",
    "pop_rate = r['pop_rate']\n",
    "rewards = r['rewards']\n",
    "values = r['values']\n",
    "action_probs = r['action_probs']\n",
    "all_lens = r['all_lens']\n",
    "all_num_balloons = r['all_num_balloons']\n",
    "\n",
    "dec_flow_scores = r['dec_flow_scores']\n",
    "\n",
    "iterators_idxs = r['iterators_idxs']\n",
    "sizes = r['sizes']\n",
    "ramp_f1s = r['ramp_f1s']\n",
    "ramp_indiv_contribs = r['ramp_indiv_contribs']\n",
    "confidence_scores = r['confidence_scores']\n",
    "unconfidence_scores = r['unconfidence_scores']\n",
    "unconfident_points = r['unconfident_points']\n",
    "step_count = r['step_count']\n",
    "all_decision_nodes = r['all_decision_nodes']\n",
    "\n",
    "cluster_regressor_coefs = r['cluster_regressor_coefs']\n",
    "cluster_regressor_scores = r['cluster_regressor_scores']\n",
    "cluster_ks = r['cluster_ks']\n",
    "pca_regressor_coefs = r['pca_regressor_coefs']\n",
    "pca_regressor_scores = r['pca_regressor_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [4:42:35<00:00, 17.66s/it]  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "# indexed by model, then layer (shared0/1, actor0/1, critic0/1), finally index\n",
    "#  first is fit to all activations, then fit to PCA i-1\n",
    "# Performance\n",
    "last_sizes = np.zeros(sizes + [17, 50])\n",
    "unpopped_sizes = np.zeros(sizes + [17, 50])\n",
    "pop_rate = np.zeros(sizes + [17])\n",
    "rewards = np.zeros(sizes + [17, 2500])\n",
    "values = np.zeros(sizes + [17, 2500])\n",
    "action_probs = np.zeros(sizes + [17, 2500])\n",
    "all_lens = np.zeros(sizes + [17])\n",
    "all_num_balloons = np.zeros(sizes + [17])\n",
    "balloon_steps = np.full(sizes + [17, 50], -1)\n",
    "button_presses = np.full(sizes + [17, 100], -1)\n",
    "\n",
    "# Decision flow\n",
    "dec_flow_scores = np.zeros(sizes + [2]) # last axis: 0=decision nodes, 1=non-dec nodes\n",
    "\n",
    "# Ramp to threshold F1 scores\n",
    "ramp_f1s = np.zeros(sizes + [6, 11]) \n",
    "ramp_indiv_contribs = np.zeros(sizes + [6, 64])\n",
    "\n",
    "# Confidence scores\n",
    "confidence_scores = np.zeros(sizes)\n",
    "unconfidence_scores = np.zeros(sizes)\n",
    "unconfident_points = np.zeros(sizes)\n",
    "step_count = np.zeros(sizes)\n",
    "all_decision_nodes = np.zeros(sizes + [17, 64])\n",
    "\n",
    "# Cluster and PCA regressor coefs\n",
    "cluster_regressor_coefs = np.zeros(sizes + [6, 3, 64])\n",
    "cluster_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "cluster_ks = np.zeros(sizes + [6])\n",
    "pca_regressor_coefs = np.zeros(sizes + [6, 3, 6])\n",
    "pca_regressor_scores = np.zeros(sizes + [6, 3])\n",
    "\n",
    "for h, i, j, k, l in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    if step_count[h, i, j, k, l] != 0:\n",
    "        continue\n",
    "    \n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chk = chks[l]\n",
    "    \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "    \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "\n",
    "    # Performance\n",
    "    lens = np.array([len(d) for d in res['dones']])\n",
    "    num_balloons = np.array([len(d) for d in res['data']['balloon_step']])\n",
    "    unpop_size = get_sizes(res, obs_rms, last_only=True)\n",
    "    for ep in range(17):\n",
    "        last_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = res['data']['last_size'][ep]\n",
    "        unpopped_sizes[h, i, j, k, l, ep, :num_balloons[ep]] = unpop_size[ep]\n",
    "        pop_rate[h, i, j, k, l, ep] = np.sum(res['data']['popped'][ep]) / num_balloons[ep]\n",
    "        values[h, i, j, k, l, ep, :lens[ep]] = res['values'][ep].reshape(-1)\n",
    "        rewards[h, i, j, k, l, ep, :lens[ep]] = res['rewards'][ep]\n",
    "        action_probs[h, i, j, k, l, ep, :lens[ep]] = res['action_probs'][ep][:, 1]\n",
    "        \n",
    "        presses = np.argwhere(res['actions'][ep].reshape(-1) == 1).reshape(-1)\n",
    "        bsteps = res['data']['balloon_step'][ep]\n",
    "        button_presses[h, i, j, k, l, ep, :len(presses)] = presses\n",
    "        balloon_steps[h, i, j, k, l, ep, :len(bsteps)] = bsteps\n",
    "        \n",
    "    all_lens[h, i, j, k, l] = lens\n",
    "    all_num_balloons[h, i, j, k, l] = num_balloons\n",
    "    \n",
    "    # Decision flow\n",
    "    try:\n",
    "        score, dec_nodes = score_decision_flow(res, model)\n",
    "        dec_flow_scores[h, i, j, k, l] = score.mean(axis=0)\n",
    "        all_decision_nodes[h, i, j, k, l] = dec_nodes\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Confidence scores    \n",
    "    non_presses = (np.vstack(res['actions']) == 0).reshape(-1)\n",
    "    presses = (np.vstack(res['actions']) == 1).reshape(-1)\n",
    "    aps = np.vstack(res['action_probs'])[:, 1]\n",
    "    confidence_scores[h, i, j, k, l] = aps[presses].mean()\n",
    "    unconfidence_scores[h, i, j, k, l] = aps[non_presses].mean()\n",
    "    unconfident_points[h, i, j, k, l] = ((aps > 0.2) & (aps < 0.8)).sum() \n",
    "    step_count[h, i, j, k, l] = len(aps)\n",
    "    \n",
    "    # Ramp to threshold scores\n",
    "    f1_scores, individual_scores = score_logistic_classifiers(res)\n",
    "    ramp_f1s[h, i, j, k, l] = f1_scores\n",
    "    ramp_indiv_contribs[h, i, j, k, l] = individual_scores\n",
    "    \n",
    "    # Cluster and PCA regressor scores\n",
    "    layers = ['shared0', 'shared1', 'actor0', 'actor1', 'critic0', 'critic1']\n",
    "    for z, layer in enumerate(layers):\n",
    "        coefs, scores = compute_regressor_coefficients(res, by_clusters=True, layer=layer)\n",
    "        n_clusters = coefs.shape[1]\n",
    "        cluster_regressor_coefs[h, i, j, k, l, z, :, :n_clusters] = coefs\n",
    "        cluster_regressor_scores[h, i, j, k, l, z] = scores\n",
    "        cluster_ks[h, i, j, k, l, z] = n_clusters\n",
    "\n",
    "        coefs, scores = compute_regressor_coefficients(res, by_clusters=False, layer=layer)\n",
    "        pca_regressor_coefs[h, i, j, k, l, z] = coefs\n",
    "        pca_regressor_scores[h, i, j, k, l, z] = scores\n",
    "    \n",
    "    \n",
    "                \n",
    "pickle.dump({\n",
    "    'last_sizes': last_sizes, \n",
    "    'unpopped_sizes': unpopped_sizes, \n",
    "    'pop_rate': pop_rate, \n",
    "    'rewards': rewards, \n",
    "    'values': values, \n",
    "    'action_probs': action_probs,\n",
    "    'all_lens': all_lens, \n",
    "    'all_num_balloons': all_num_balloons, \n",
    "    'balloon_steps': balloon_steps,\n",
    "    'button_presses': button_presses,\n",
    "    \n",
    "    'dec_flow_scores': dec_flow_scores,\n",
    "    \n",
    "    'iterators_idxs': iterators_idxs,\n",
    "    'sizes': sizes,\n",
    "    'ramp_f1s': ramp_f1s,\n",
    "    'ramp_indiv_contribs': ramp_indiv_contribs,\n",
    "    'confidence_scores': confidence_scores,\n",
    "    'unconfidence_scores': unconfidence_scores,\n",
    "    'unconfident_points': unconfident_points,\n",
    "    'step_count': step_count,\n",
    "    'all_decision_nodes': all_decision_nodes,\n",
    "    \n",
    "    'cluster_regressor_coefs': cluster_regressor_coefs, \n",
    "    'cluster_regressor_scores': cluster_regressor_scores, \n",
    "    'cluster_ks': cluster_ks, \n",
    "    'pca_regressor_coefs': pca_regressor_coefs, \n",
    "    'pca_regressor_scores': pca_regressor_scores, \n",
    "}, open('data/meta_representation_results', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 960/960 [34:25<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main section to collect a bunch of data on confidence, activations\n",
    "and ramping signal\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "give_rew = ['', 'giverew_']\n",
    "postfixes = ['', 'pop0.05', 'pop0.1', 'pop0.2']\n",
    "models = [1.0, 1.2, 1.5, 1.7, 2.0]\n",
    "trials = range(3)\n",
    "chks = np.arange(10, 243, 30)\n",
    "\n",
    "iterators = [give_rew, postfixes, models, trials, chks]\n",
    "iterators_idxs = [range(len(i)) for i in iterators]\n",
    "sizes = [len(i) for i in iterators]\n",
    "\n",
    "pop_rate = np.zeros(sizes + [17])\n",
    "\n",
    "for h, i, j, k, l in tqdm(itertools.product(*iterators_idxs), total=np.prod(sizes)):\n",
    "    give = give_rew[h]\n",
    "    postfix = postfixes[i]\n",
    "    model = models[j]\n",
    "    t = k\n",
    "    chk = chks[l]\n",
    "    \n",
    "    if h == 1 and postfix == '':\n",
    "        postfix = 'pop0'\n",
    "    exp_name = f\"{give}p{model}n50{postfix}\"\n",
    "    model, (obs_rms, ret_rms) = \\\n",
    "        torch.load(f'../saved_checkpoints/meta_v2/{exp_name}_{t}/{chk}.pt')\n",
    "    \n",
    "    if h == 1:\n",
    "        res = giverew_evalu(model, obs_rms)\n",
    "    else:\n",
    "        res = evalu(model, obs_rms)\n",
    "    res = reshape_parallel_evalu_res(res, meta_balloons=50)\n",
    "\n",
    "    # Performance\n",
    "    lens = np.array([len(d) for d in res['dones']])\n",
    "    num_balloons = np.array([len(d) for d in res['data']['balloon_step']])\n",
    "    unpop_size = get_sizes(res, obs_rms, last_only=True)\n",
    "    for ep in range(17):\n",
    "        pop_rate[h, i, j, k, l, ep] = np.sum(res['data']['popped'][ep]) / num_balloons[ep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pickle.load(open('data/meta_representation_results', 'rb'))\n",
    "last_sizes = r['last_sizes'] \n",
    "unpopped_sizes = r['unpopped_sizes'] \n",
    "pop_rate = r['pop_rate'] \n",
    "rewards = r['rewards'] \n",
    "values = r['values'] \n",
    "action_probs = r['action_probs']\n",
    "all_lens = r['all_lens'] \n",
    "all_num_balloons = r['all_num_balloons'] \n",
    "dec_flow_scores = r['dec_flow_scores']\n",
    "iterators_idxs = r['iterators_idxs']\n",
    "sizes = r['sizes']\n",
    "ramp_f1s = r['ramp_f1s']\n",
    "ramp_indiv_contribs = r['ramp_indiv_contribs']\n",
    "confidence_scores = r['confidence_scores']\n",
    "unconfidence_scores = r['unconfidence_scores']\n",
    "unconfident_points = r['unconfident_points']\n",
    "step_count = r['step_count']\n",
    "all_decision_nodes = r['all_decision_nodes']\n",
    "cluster_regressor_coefs = r['cluster_regressor_coefs'] \n",
    "cluster_regressor_scores = r['cluster_regressor_scores'] \n",
    "cluster_ks = r['cluster_ks'] \n",
    "pca_regressor_coefs = r['pca_regressor_coefs'] \n",
    "pca_regressor_scores = r['pca_regressor_scores'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({\n",
    "    'last_sizes': last_sizes, \n",
    "    'unpopped_sizes': unpopped_sizes, \n",
    "    'pop_rate': pop_rate, \n",
    "    'rewards': rewards, \n",
    "    'values': values, \n",
    "    'action_probs': action_probs,\n",
    "    'all_lens': all_lens, \n",
    "    'all_num_balloons': all_num_balloons, \n",
    "    \n",
    "    'dec_flow_scores': dec_flow_scores,\n",
    "    \n",
    "    'iterators_idxs': iterators_idxs,\n",
    "    'sizes': sizes,\n",
    "    'ramp_f1s': ramp_f1s,\n",
    "    'ramp_indiv_contribs': ramp_indiv_contribs,\n",
    "    'confidence_scores': confidence_scores,\n",
    "    'unconfidence_scores': unconfidence_scores,\n",
    "    'unconfident_points': unconfident_points,\n",
    "    'step_count': step_count,\n",
    "    'all_decision_nodes': all_decision_nodes,\n",
    "    \n",
    "    'cluster_regressor_coefs': cluster_regressor_coefs, \n",
    "    'cluster_regressor_scores': cluster_regressor_scores, \n",
    "    'cluster_ks': cluster_ks, \n",
    "    'pca_regressor_coefs': pca_regressor_coefs, \n",
    "    'pca_regressor_scores': pca_regressor_scores, \n",
    "}, open('data/meta_representation_results', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
